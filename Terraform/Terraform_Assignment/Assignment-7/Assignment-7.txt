Using Join and Count to Create Multiple S3 Buckets

Find three files as below:-

main.tf :-
---------
provider "aws" {
  region = "${var.aws_region}"
}

resource "random_id" "tf_bucket_id" {
  byte_length = 2
}

resource "aws_s3_bucket" "tf_code" {
    bucket        = "${var.project_name}-${random_id.tf_bucket_id.dec}"
    acl           = "private"

    force_destroy =  true

    tags {
      Name = "tf_bucket"
    }
}


variables.tf:-
--------------
variable "aws_region" {
  default = "us-east-1"
}

variable "project_name" {
  default = "la-terraform"
}


outputs.tf:-
-------------
output "bucketname" {
  value = "${aws_s3_bucket.tf_code.id}"
}



Update the Variables File:-
Edit variables.tf.
Add a new variable number_of_instances.
Set the the default to 2.

Update the Main File:-
Update random_id and add a count.
Set the value count to use the number_of_instances variable.
Update aws_s3_bucket and add a count.
Update random_id.tf_bucket_id.dec so it iterates through the count. Update the Name tag so that tf_bucket is appended with the count index plus one.

Update the Outputs File:-
Update the bucketname output value to use the join function so that it returns a comma delimited list of bucket names.

Deploy the Infrastructure:-
Initialize Terraform.
Validate the files.
Deploy the S3 buckets.



****************************************************************************************************************************


VinayKumar@DESKTOP-MFHQUNG MINGW64 ~/Desktop/New_folder/Terraform/Terraform/Assignment-7 (dev)$ export AWS_ACCESS_KEY_ID="AKIAQE453C7KMNT4GYUD"

VinayKumar@DESKTOP-MFHQUNG MINGW64 ~/Desktop/New_folder/Terraform/Terraform/Assignment-7 (dev)
$ export AWS_SECRET_ACCESS_KEY="E70pZkoBlmH2T5oRBSzsPbvXbfN9b2sBtaSWoJpg"

VinayKumar@DESKTOP-MFHQUNG MINGW64 ~/Desktop/New_folder/Terraform/Terraform/Assignment-7 (dev)$ terraform init                                                                                                                                                Initializing the backend...                                                                                                                                     Initializing provider plugins...                                                - Reusing previous version of hashicorp/aws from the dependency lock file       - Reusing previous version of hashicorp/random from the dependency lock file    - Using previously-installed hashicorp/aws v3.55.0                              - Using previously-installed hashicorp/random v3.1.0                                                                                                            Terraform has been successfully initialized!                                                                                                                    You may now begin working with Terraform. Try running "terraform plan" to see   any changes that are required for your infrastructure. All Terraform commands
should now work.

If you ever set or change modules or backend configuration for Terraform,
rerun this command to reinitialize your working directory. If you forget, other commands will detect it and remind you to do so if necessary.

VinayKumar@DESKTOP-MFHQUNG MINGW64 ~/Desktop/New_folder/Terraform/Terraform/Assignment-7 (dev)
$ terraform validate
Success! The configuration is valid.


VinayKumar@DESKTOP-MFHQUNG MINGW64 ~/Desktop/New_folder/Terraform/Terraform/Assignment-7 (dev)
$ terraform plan -out s3_multiple.tfplan

Terraform used the selected providers to generate the following execution plan. Resource actions are indicated with the following symbols:
  + create

Terraform will perform the following actions:

  # aws_s3_bucket.tf_code[0] will be created
  + resource "aws_s3_bucket" "tf_code" {
      + acceleration_status         = (known after apply)
      + acl                         = "private"
      + arn                         = (known after apply)
      + bucket                      = (known after apply)
      + bucket_domain_name          = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = true
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + region                      = (known after apply)
      + request_payer               = (known after apply)
      + tags                        = {
          + "Name" = "tf_bucket-0"
        }
      + tags_all                    = {
          + "Name" = "tf_bucket-0"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)

      + versioning {
          + enabled    = (known after apply)
          + mfa_delete = (known after apply)
        }
    }

  # aws_s3_bucket.tf_code[1] will be created
  + resource "aws_s3_bucket" "tf_code" {
      + acceleration_status         = (known after apply)
      + acl                         = "private"
      + arn                         = (known after apply)
      + bucket                      = (known after apply)
      + bucket_domain_name          = (known after apply)
      + bucket_regional_domain_name = (known after apply)
      + force_destroy               = true
      + hosted_zone_id              = (known after apply)
      + id                          = (known after apply)
      + region                      = (known after apply)
      + request_payer               = (known after apply)
      + tags                        = {
          + "Name" = "tf_bucket-1"
        }
      + tags_all                    = {
          + "Name" = "tf_bucket-1"
        }
      + website_domain              = (known after apply)
      + website_endpoint            = (known after apply)

      + versioning {
          + enabled    = (known after apply)
          + mfa_delete = (known after apply)
        }
    }

  # random_id.tf_bucket_id[0] will be created
  + resource "random_id" "tf_bucket_id" {
      + b64_std     = (known after apply)
      + b64_url     = (known after apply)
      + byte_length = 2
      + dec         = (known after apply)
      + hex         = (known after apply)
      + id          = (known after apply)
    }

  # random_id.tf_bucket_id[1] will be created
  + resource "random_id" "tf_bucket_id" {
      + b64_std     = (known after apply)
      + b64_url     = (known after apply)
      + byte_length = 2
      + dec         = (known after apply)
      + hex         = (known after apply)
      + id          = (known after apply)
    }

Plan: 4 to add, 0 to change, 0 to destroy.

Changes to Outputs:
  + bucketname = (known after apply)

───────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────────

Saved the plan to: s3_multiple.tfplan

To perform exactly these actions, run the following command to apply:
    terraform apply "s3_multiple.tfplan"

VinayKumar@DESKTOP-MFHQUNG MINGW64 ~/Desktop/New_folder/Terraform/Terraform/Assignment-7 (dev)
$ terraform apply "s3_multiple.tfplan"
random_id.tf_bucket_id[1]: Creating...
random_id.tf_bucket_id[0]: Creating...
random_id.tf_bucket_id[1]: Creation complete after 0s [id=CGg]
random_id.tf_bucket_id[0]: Creation complete after 0s [id=b_A]
aws_s3_bucket.tf_code[0]: Creating...
aws_s3_bucket.tf_code[1]: Creating...
aws_s3_bucket.tf_code[0]: Still creating... [10s elapsed]
aws_s3_bucket.tf_code[1]: Still creating... [10s elapsed]
aws_s3_bucket.tf_code[0]: Still creating... [20s elapsed]
aws_s3_bucket.tf_code[1]: Still creating... [20s elapsed]
aws_s3_bucket.tf_code[1]: Creation complete after 22s [id=la-terraform-2152]
aws_s3_bucket.tf_code[0]: Creation complete after 22s [id=la-terraform-28656]

Apply complete! Resources: 4 added, 0 changed, 0 destroyed.

Outputs:

bucketname = "la-terraform-28656,la-terraform-2152"
